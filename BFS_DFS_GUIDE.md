# BFS vs DFS Crawling Guide

## âœ… BFS is Now Enabled!

Your crawler is now set to use **Breadth-First Search (BFS)** instead of Depth-First Search (DFS).

## ğŸ”„ What Changed?

### Updated Files:
1. **`config.json`** - Added `"USE_BFS": true`
2. **`src/crawlers/CrawlerCreator.py`** - Added BFS/DFS configuration logic

## ğŸ“Š BFS vs DFS: What's the Difference?

### **Depth-First Search (DFS)** - Default Scrapy Behavior
```
Homepage
  â””â”€> Link 1
      â””â”€> Link 1.1
          â””â”€> Link 1.1.1  â† Goes deep first
              â””â”€> Link 1.1.1.1
```

**Characteristics:**
- âœ… Follows links deeply before going wide
- âœ… Good for finding specific deep content quickly
- âŒ May miss important top-level pages if interrupted
- âŒ Can get stuck in deep link chains

**Example:** Starts at homepage â†’ About page â†’ Staff directory â†’ Individual staff page â†’ That person's publications â†’ etc.

---

### **Breadth-First Search (BFS)** - Now Enabled!
```
Homepage
  â”œâ”€> Link 1        â† Crawls all links at same level first
  â”œâ”€> Link 2
  â”œâ”€> Link 3
  â””â”€> Link 4
      â”œâ”€> Link 1.1
      â”œâ”€> Link 1.2
      â””â”€> Link 1.3
```

**Characteristics:**
- âœ… Crawls all pages at same depth before going deeper
- âœ… Gets important top-level content first
- âœ… Better for interrupted crawls (saves important pages first)
- âœ… More predictable crawl order

**Example:** Starts at homepage â†’ All main sections (About, Admissions, Academics) â†’ All subsections â†’ Individual pages

## ğŸ¯ When to Use Each

### Use **BFS** (Current Setting) When:
- âœ… You want to prioritize top-level pages
- âœ… You have page limits (`CLOSESPIDER_PAGECOUNT`)
- âœ… You want broader coverage before depth
- âœ… Crawl might be interrupted
- âœ… **You want the most important pages first** â­

### Use **DFS** When:
- âœ… You need to fully explore specific sections
- âœ… No page limits (crawling entire site)
- âœ… Deep content is more valuable than broad content
- âœ… Following specific link chains

## ğŸ”§ How to Switch

### Enable BFS (Current):
```json
{
  "settings": {
    "USE_BFS": true
  }
}
```

### Enable DFS:
```json
{
  "settings": {
    "USE_BFS": false
  }
}
```

Or simply remove the line (DFS is default).

## ğŸ“ˆ Practical Example: CU Boulder

### With BFS (âœ… Current):
```
Crawl Order:
1. https://www.colorado.edu/
2. https://www.colorado.edu/academics
3. https://www.colorado.edu/admissions
4. https://www.colorado.edu/about
5. https://www.colorado.edu/campus-life
...then deeper pages in each section
```

**Result:** Gets overview of entire university before diving into details.

### With DFS:
```
Crawl Order:
1. https://www.colorado.edu/
2. https://www.colorado.edu/academics
3. https://www.colorado.edu/academics/programs
4. https://www.colorado.edu/academics/programs/computer-science
5. https://www.colorado.edu/academics/programs/computer-science/courses
...deeply explores academics before moving to admissions
```

**Result:** Fully explores one section before moving to next.

## ğŸ“ For Your Use Case

Since you have `"CLOSESPIDER_PAGECOUNT": 1000` (stops after 1000 pages), **BFS is better** because:

1. âœ… **Broader coverage** - Gets pages from all major sections
2. âœ… **Better RAG results** - More diverse content for search
3. âœ… **More important pages first** - Main pages before deep details
4. âœ… **Interrupted crawls** - Still have valuable data if stopped early

## ğŸ“Š Technical Details

What the code actually changes:

```python
# BFS Settings (USE_BFS: true)
DEPTH_PRIORITY = 1  # Higher depth = higher priority
SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'  # FIFO = First In First Out
SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'

# DFS Settings (USE_BFS: false)
DEPTH_PRIORITY = 0  # Depth doesn't affect priority
SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'  # LIFO = Last In First Out
SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'
```

## ğŸ§ª Testing

You can verify BFS is working by checking your logs:

```bash
# Start a crawl and watch the URLs
python main.py | grep "Scraped:"

# BFS will show pages at similar depth levels clustered together
# DFS will show deep chains of related pages
```

## ğŸ’¡ Pro Tip

You can combine BFS with `DEPTH_LIMIT` for optimal coverage:

```json
{
  "settings": {
    "USE_BFS": true,
    "DEPTH_LIMIT": 3,  // Only go 3 levels deep
    "CLOSESPIDER_PAGECOUNT": 1000
  }
}
```

This ensures you get:
- âœ… Top 3 levels of entire site
- âœ… No wasted crawls on very deep pages
- âœ… Maximum breadth within reasonable depth

---

**Your crawler is now optimized with BFS!** ğŸš€
